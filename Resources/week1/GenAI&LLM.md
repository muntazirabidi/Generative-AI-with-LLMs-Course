## Generative AI:

- Subset of traditional machine learning.
- Machines mimic or approximate human abilities by creating content.
- Models find statistical patterns in huge datasets initially generated by humans.


## Large Language Models:

- Trained on trillions of words over extended periods.
- Require substantial compute power.
- "Foundation models" or "base models" have billions of parameters.
- Think of parameters as the model's "memory."
- More parameters mean more memory and sophistication in tasks.
- Exhibit properties beyond just language, including reasoning and problem-solving.

## Foundation Models:

- Different models have different sizes based on parameters.
- Throughout the course, LLMs are represented by purple circles.
- In labs, the open-source model "flan-T5" will be used.
- Models can be fine-tuned for specific use cases, eliminating the need to train from scratch.

## Course Focus:

- Mainly on LLMs and their application in natural language generation.
- Covers how LLMs are built, trained, interacted with via prompts, and deployed for business/social tasks.
- Unlike other programming paradigms, LLMs take natural language or human-written instructions.

## Interacting with LLMs:

- The text passed to LLMs is termed a "prompt."
- "Context window" refers to the memory available for a prompt.
- Example provided: Asking about Ganymede's location.
- The model processes the prompt and predicts next words.
- The output (answer to the question) is termed "completion."
- The act of using the model to generate text is called "inference."
