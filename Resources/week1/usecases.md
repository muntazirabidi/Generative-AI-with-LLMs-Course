## Common Misconception:
- People often associate LLMs and generative AI mainly with chat tasks due to the visibility of chatbots.

## Base Concept: Next Word Prediction:
- Foundation for various capabilities beyond basic chatbots.

## Applications of LLMs:
### Text Generation:
- Compose essays based on prompts.
- Summarize dialogues/conversations.

### Translation Tasks:
- Conventional language translations (e.g., French to German, English to Spanish).
- Translate natural language to machine code (e.g., requesting Python code).

### Focused Tasks:
- Information retrieval.
- Named entity recognition (e.g., identifying people and places in a news article).

### Augmentation and Extension:
- Connect LLMs to external data sources or external APIs.
- Enables real-world interactions and supplements pre-trained knowledge.

## Model Scale vs. Language Understanding:
- As foundation model scale grows (from millions to hundreds of billions of parameters):
   - The model's subjective understanding of language deepens.
   - Knowledge encoded within model parameters facilitates reasoning and task execution.

- Smaller models can still be optimized for specific, focused tasks.

## Course Insights:
- Week 2: Fine-tuning and optimizing smaller models for specific tasks.
- Week 3: Augmenting LLMs with external data sources and invoking external APIs.

## LLM Capability Evolution:
The recent boost in LLM capabilities is due to their underlying architecture.
