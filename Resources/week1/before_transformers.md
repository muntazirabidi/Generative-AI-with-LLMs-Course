## Introduction to Generative Algorithms
- Generative algorithms have been around for a while, they are not new.

## Previous Generations: RNNs
- **Recurrent Neural Networks (RNNs):** A type of neural network architecture used by older language models.
- Limitations of RNNs:
   - Restricted by compute and memory capacities.
   - Scaling RNNs requires more resources.
- Challenges in predicting next words based on limited preceding words.

## Language Complexity Challenges
- **Homonyms:** Words with multiple meanings (e.g., "bank" could mean a financial institution or the side of a river).
- **Syntactic Ambiguity:** Uncertainty in sentence structures, leading to varied interpretations (e.g., "The teacher taught the students with the book." - Who had the book?).

## The Evolution: Transformer Architecture
- Introduced in the 2017 paper, "Attention is All You Need" by Google and the University of Toronto.

## Key Features:
- Efficient scaling with multi-core GPUs.
- Parallel processing of input data.
- Ability to handle larger training datasets.
- Focuses on the "attention mechanism" which understands word meanings by considering the context.

# Domain Terms Explained:
1. **Generative Algorithms:** Algorithms capable of generating new data that resembles some given set of data. In the context of language, these algorithms can produce coherent, contextually relevant sentences.

2.** Recurrent Neural Networks (RNNs):** A type of neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, etc. They are called 'recurrent' because they perform the same task for every element of a sequence.

3. **Homonyms:** Words that are spelled and pronounced the same way but have different meanings.

4. **Syntactic Ambiguity:** A situation where a sentence can reasonably be interpreted in more than one way due to its structure or punctuation.

5. **Transformer Architecture:** A type of model architecture that revolutionized machine learning tasks, particularly in Natural Language Processing (NLP). The transformer model's attention mechanism allows it to focus on different parts of the input text, providing better context and understanding.

6. **Attention Mechanism:** In deep learning, especially in NLP, attention mechanisms let a model focus on certain parts of the input, similar to how humans pay attention to specific parts of input when understanding languages, reading, etc.
